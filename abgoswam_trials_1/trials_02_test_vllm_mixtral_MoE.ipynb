{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/harveyp123/DPR-hongwu/blob/main/my_DPR_try/test_vllm_mixtral_MoE.ipynb\n",
    "# https://github.com/harveyp123/DPR-hongwu/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # define GPU id, remove if you want to use all GPUs available\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 21:08:20,581\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 21:08:21 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO', tokenizer='NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='../models_pretrain/', load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerVllm pid=3993788)\u001b[0m INFO 04-10 21:08:33 selector.py:16] Using FlashAttention backend.\n",
      "INFO 04-10 21:08:33 selector.py:16] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerVllm pid=3994131)\u001b[0m INFO 04-10 21:08:34 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "INFO 04-10 21:08:35 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerVllm pid=3993788)\u001b[0m INFO 04-10 21:08:47 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerVllm pid=3994028)\u001b[0m INFO 04-10 21:08:33 selector.py:16] Using FlashAttention backend.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayWorkerVllm pid=3994028)\u001b[0m INFO 04-10 21:08:35 pynccl_utils.py:45] vLLM is using nccl==2.18.1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 04-10 21:08:47 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 04-10 21:08:53 model_runner.py:104] Loading model weights took 21.7576 GB\n",
      "\u001b[36m(RayWorkerVllm pid=3993788)\u001b[0m INFO 04-10 21:08:54 model_runner.py:104] Loading model weights took 21.7576 GB\n",
      "\u001b[36m(RayWorkerVllm pid=3994131)\u001b[0m INFO 04-10 21:08:47 weight_utils.py:177] Using model weights format ['*.safetensors']\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 04-10 21:09:04 ray_gpu_executor.py:240] # GPU blocks: 41411, # CPU blocks: 8192\n",
      "INFO 04-10 21:09:08 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-10 21:09:08 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=3993788)\u001b[0m INFO 04-10 21:09:08 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=3993788)\u001b[0m INFO 04-10 21:09:08 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=3994131)\u001b[0m INFO 04-10 21:08:55 model_runner.py:104] Loading model weights took 21.7576 GB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 04-10 21:09:18 model_runner.py:867] Graph capturing finished in 10 secs.\n",
      "\u001b[36m(RayWorkerVllm pid=3993788)\u001b[0m INFO 04-10 21:09:18 model_runner.py:867] Graph capturing finished in 10 secs.\n",
      "\u001b[36m(RayWorkerVllm pid=3994131)\u001b[0m INFO 04-10 21:09:08 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerVllm pid=3994131)\u001b[0m INFO 04-10 21:09:08 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "num_gpus = 4\n",
    "# model = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# mode = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "model = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "\n",
    "llm = LLM(model=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\", dtype=\"bfloat16\", gpu_memory_utilization=0.96, tensor_parallel_size = num_gpus, download_dir = '../models_pretrain/')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, cache_dir = '../models_pretrain/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation using base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'How are you today? ', Generated text: 'ðŸ™‚\\n\\nI want to share with you a story from my personal life.\\n\\nI spent'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Base model generation\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens= 20, ignore_eos=False)\n",
    "prompt = \"How are you today? \"\n",
    "outputs = llm.generate(prompt, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation using chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a friendly chatbot who always responds in a polite fashion<|im_end|>\n",
      "<|im_start|>user\n",
      "How many helicopters can a human eat in one sitting?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "'<|im_start|>system\\nYou are a friendly chatbot who always responds in a polite fashion<|im_end|>\\n<|im_start|>user\\nHow many helicopters can a human eat in one sitting?<|im_end|>\\n<|im_start|>assistant\\n' \n",
      " Generated text: \n",
      "'I would say that humans are not physiologically designed to consume helicopters in any sitting. Helicopters are large, complex machines made of metal and other materials, which are not edible. Please remember to stick to a healthy and balanced diet.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens= 200, ignore_eos=False)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in a polite fashion\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "\n",
    "#### Reference: https://huggingface.co/docs/transformers/main/chat_templating\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "outputs = llm.generate(prompt, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: \\n{prompt!r} \\n Generated text: \\n{generated_text!r}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starbuck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
